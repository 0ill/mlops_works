{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Management and Feature Engineering\n",
    "Importance: Data is the foundation of machine learning models. Proper management ensures high-quality, relevant data for training. Feature engineering transforms raw data into features that improve model performance.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Better model accuracy through relevant features.\n",
    "- Reduced noise in data, leading to more robust models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to optimize the dataframe and possibly reduce size\n",
    "def optimize_data_types(data):\n",
    "    \"\"\"Optimize data types to reduce memory usage.\"\"\"\n",
    "    for column in data.columns:\n",
    "        column_type = data[column].dtype\n",
    "\n",
    "        # Check for numeric types\n",
    "        if pd.api.types.is_numeric_dtype(column_type):\n",
    "            if pd.api.types.is_integer_dtype(column_type):\n",
    "                # Downcast integer types\n",
    "                data[column] = pd.to_numeric(data[column], downcast='integer')\n",
    "            elif pd.api.types.is_float_dtype(column_type):\n",
    "                # Downcast float types\n",
    "                data[column] = pd.to_numeric(data[column], downcast='float')\n",
    "\n",
    "        # Check for categorical types\n",
    "        elif pd.api.types.is_object_dtype(column_type):\n",
    "            unique_values = data[column].nunique()\n",
    "            total_values = data[column].size\n",
    "            if unique_values / total_values < 0.5:  # 50% unique values threshold\n",
    "                # Convert to category if there are few unique values\n",
    "                data[column] = data[column].astype('category')\n",
    "            else:\n",
    "                # Convert to the new string type for better performance\n",
    "                data[column] = data[column].astype('string')\n",
    "\n",
    "        # Check for boolean types\n",
    "        elif pd.api.types.is_bool_dtype(column_type):\n",
    "            # Convert booleans to a more memory-efficient representation if needed\n",
    "            data[column] = data[column].astype('bool')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Machine (In House)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Invoice Date</th>\n",
       "      <th>Price per Unit</th>\n",
       "      <th>Units Sold</th>\n",
       "      <th>Total Sales</th>\n",
       "      <th>Operating Profit</th>\n",
       "      <th>Operating Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York_Foot Locker_Men's Street Footwear_Nor...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>300000.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York_Foot Locker_Men's Street Footwear_Nor...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>47.0</td>\n",
       "      <td>336</td>\n",
       "      <td>15792.0</td>\n",
       "      <td>9633.12</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York_Foot Locker_Men's Street Footwear_Nor...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>34.0</td>\n",
       "      <td>384</td>\n",
       "      <td>13056.0</td>\n",
       "      <td>6789.12</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Philadelphia_Foot Locker_Women's Apparel_North...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>68.0</td>\n",
       "      <td>83</td>\n",
       "      <td>5644.0</td>\n",
       "      <td>2426.92</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philadelphia_Foot Locker_Women's Apparel_North...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>128.0</td>\n",
       "      <td>358</td>\n",
       "      <td>210649.0</td>\n",
       "      <td>63282.68</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ID Invoice Date  \\\n",
       "0  New York_Foot Locker_Men's Street Footwear_Nor...   2020-01-01   \n",
       "1  New York_Foot Locker_Men's Street Footwear_Nor...   2020-01-01   \n",
       "2  New York_Foot Locker_Men's Street Footwear_Nor...   2020-01-01   \n",
       "3  Philadelphia_Foot Locker_Women's Apparel_North...   2020-01-01   \n",
       "4  Philadelphia_Foot Locker_Women's Apparel_North...   2020-01-01   \n",
       "\n",
       "   Price per Unit  Units Sold  Total Sales  Operating Profit  Operating Margin  \n",
       "0            50.0        1200     600000.0         300000.00              0.50  \n",
       "1            47.0         336      15792.0           9633.12              0.61  \n",
       "2            34.0         384      13056.0           6789.12              0.52  \n",
       "3            68.0          83       5644.0           2426.92              0.43  \n",
       "4           128.0         358     210649.0          63282.68              0.62  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local Machine\n",
    "data_org = pd.read_excel('../data/Adidas.xlsx',header=4,).drop(['Unnamed: 0'], axis=1)\n",
    "data_org['Retailer ID'] = data_org['Retailer ID'].astype(\"string\")\n",
    "data_org = data_org.groupby([\"Invoice Date\",'City','Retailer','Product','Region','Retailer ID','State','Sales Method']).sum()\n",
    "# to optimize dataframe\n",
    "data_org = optimize_data_types(data_org)\n",
    "#data_org.tail()\n",
    "\n",
    "data_org = data_org.reset_index(level=0)\n",
    "data_org.columns\n",
    "\n",
    "data_org.index = data_org.index.map('_'.join)\n",
    "data_org\n",
    "\n",
    "data_org = data_org.reset_index().rename(columns={\"index\":\"ID\"})\n",
    "data_org.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML\n",
    "- **Services**: Data preparation services, Azure Data Factory.\n",
    "- **Pros**: Integrated data services; Azure Data Factory allows for complex data workflows.\n",
    "- **Cons**: Some features may have a steep learning curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-identity in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (1.19.0)\n",
      "Requirement already satisfied: azure-mgmt-resource in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (23.1.1)\n",
      "Requirement already satisfied: azure-mgmt-sql in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: azure-mgmt-storage in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (21.2.1)\n",
      "Requirement already satisfied: azure-mgmt-datafactory in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: azure-storage-file-share in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (12.19.0)\n",
      "Requirement already satisfied: azure-ai-ml in /Users/0ill/anaconda3/envs/oill/lib/python3.9/site-packages (1.21.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement azure-mgmt-machinelearning (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for azure-mgmt-machinelearning\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install azure-identity azure-mgmt-resource azure-mgmt-sql azure-mgmt-storage azure-mgmt-datafactory azure-storage-file-share azure-ai-ml pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.sql import SqlManagementClient\n",
    "from azure.storage.fileshare import ShareServiceClient\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "#from azure.mgmt.ml import MachineLearningServiceClient\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Workspace\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml import dsl, load_component\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure credentials and configuration\n",
    "credential = DefaultAzureCredential()\n",
    "subscription_id = '8f7e0e9e-aa19-4956-8137-6ca807cad266'  #'your_subscription_id'\n",
    "resource_group_name = 'azur_ml' #'your_resource_group_name'\n",
    "location = 'centralus' # your_location\n",
    "svr_name = 'azurmlserver'#'your-sql-server-name'\n",
    "db_name  = 'azurmldb'#'your-database-name'\n",
    "admin_login = 'azurmlusername'#'your_admin_username'\n",
    "admin_login_pass = 'azurmlpassword$abc123:911' #'your_admin_password'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Resource Management client\n",
    "resource_client = ResourceManagementClient(credential, subscription_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_resource_group(resource_client):\n",
    "    print(f\"Creating resource group '{resource_group_name}'...\")\n",
    "    resource_group = resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {'location': location}\n",
    "    )\n",
    "    print(f\"Resource group '{resource_group_name}' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating resource group 'azur_ml'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group 'azur_ml' created.\n"
     ]
    }
   ],
   "source": [
    "create_resource_group(resource_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_resource_group(resource_client):\n",
    "    print(f\"Deleting resource group '{resource_group_name}'...\")\n",
    "    resource_client.resource_groups.begin_delete(resource_group_name).result()\n",
    "    print(f\"Resource group '{resource_group_name}' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting resource group 'azur_ml'...\n",
      "Resource group 'azur_ml' deleted.\n"
     ]
    }
   ],
   "source": [
    "delete_resource_group(resource_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sql_database(credential, subscription_id, svr_name, db_name, admin_login, admin_login_pass):\n",
    "    sql_client = SqlManagementClient(credential, subscription_id)\n",
    "    server_name = svr_name #'your-sql-server-name'\n",
    "    database_name = db_name #'your-database-name'\n",
    "\n",
    "    print(f\"Creating SQL server '{server_name}'...\")\n",
    "    server = sql_client.servers.begin_create_or_update(\n",
    "        resource_group_name,\n",
    "        server_name,\n",
    "        {\n",
    "            'location': location,\n",
    "            'administrator_login': admin_login, #'your_admin_username',\n",
    "            'administrator_login_password': admin_login_pass #'your_admin_password'\n",
    "        }\n",
    "    ).result()\n",
    "\n",
    "    print(f\"Creating SQL database '{database_name}'...\")\n",
    "    database = sql_client.databases.begin_create_or_update(\n",
    "        resource_group_name,\n",
    "        server_name,\n",
    "        database_name,\n",
    "        {\n",
    "            'location': location,\n",
    "            'sku': {'name': 'S0'}\n",
    "        }\n",
    "    ).result()\n",
    "    print(f\"SQL database '{database_name}' created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SQL server 'azurmlserver'...\n",
      "Creating SQL database 'azurmldb'...\n",
      "SQL database 'azurmldb' created.\n"
     ]
    }
   ],
   "source": [
    "create_sql_database(credential, subscription_id, svr_name, db_name, admin_login, admin_login_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete SQL Server\n",
    "def delete_sql_server(credential, subscription_id, resource_group_name, server_name):\n",
    "    sql_client = SqlManagementClient(credential, subscription_id)\n",
    "    server_name = 'your_server_name'  # Same name used for creation\n",
    "\n",
    "    print(f\"Deleting SQL server '{server_name}'...\")\n",
    "    sql_client.servers.begin_delete(\n",
    "        resource_group_name,\n",
    "        server_name\n",
    "    ).wait()\n",
    "    print(f\"SQL server '{server_name}' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storage_account_and_file_share():\n",
    "    storage_client = StorageManagementClient(credential, subscription_id)\n",
    "    storage_account_name = 'yourstorageaccount'\n",
    "    file_share_name = 'your-file-share'\n",
    "\n",
    "    print(f\"Creating storage account '{storage_account_name}'...\")\n",
    "    storage_account = storage_client.storage_accounts.begin_create(\n",
    "        resource_group_name,\n",
    "        storage_account_name,\n",
    "        {\n",
    "            'location': location,\n",
    "            'kind': 'StorageV2',\n",
    "            'sku': {'name': 'Standard_LRS'}\n",
    "        }\n",
    "    ).result()\n",
    "\n",
    "    print(f\"Creating file share '{file_share_name}'...\")\n",
    "    storage_client.file_shares.create(\n",
    "        resource_group_name,\n",
    "        storage_account_name,\n",
    "        file_share_name,\n",
    "        {}\n",
    "    )\n",
    "    print(f\"File share '{file_share_name}' created.\")\n",
    "\n",
    "def create_data_factory():\n",
    "    adf_client = DataFactoryManagementClient(credential, subscription_id)\n",
    "    factory_name = 'your-data-factory-name'\n",
    "\n",
    "    print(f\"Creating Data Factory '{factory_name}'...\")\n",
    "    factory = adf_client.factories.begin_create_or_update(\n",
    "        resource_group_name,\n",
    "        factory_name,\n",
    "        {\n",
    "            'location': location\n",
    "        }\n",
    "    ).result()\n",
    "    print(f\"Data Factory '{factory_name}' created.\")\n",
    "\n",
    "def create_ml_workspace():\n",
    "    ml_client = MachineLearningServiceClient(credential, subscription_id)\n",
    "    workspace_name = 'your-ml-workspace-name'\n",
    "\n",
    "    print(f\"Creating Machine Learning workspace '{workspace_name}'...\")\n",
    "    workspace = ml_client.workspaces.begin_create_or_update(\n",
    "        resource_group_name,\n",
    "        workspace_name,\n",
    "        {\n",
    "            'location': location,\n",
    "            'sku': {'name': 'Basic', 'tier': 'Basic'},\n",
    "            'identity': {'type': 'SystemAssigned'}\n",
    "        }\n",
    "    ).result()\n",
    "    print(f\"Machine Learning workspace '{workspace_name}' created.\")\n",
    "\n",
    "def perform_data_preparation():\n",
    "    # This is a simplified example. In a real scenario, you would use Azure ML SDK\n",
    "    # to create a dataset, define a data preparation step, and run an experiment.\n",
    "    print(\"Performing data preparation...\")\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group_name, 'your-ml-workspace-name')\n",
    "    \n",
    "    # Create a simple dataset (this is just a placeholder)\n",
    "    dataset = ml_client.data.create_or_update(\n",
    "        name='your-dataset-name',\n",
    "        path='path/to/your/data',\n",
    "        type='uri_file'\n",
    "    )\n",
    "    \n",
    "    print(\"Data preparation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_resource_group()\n",
    "    create_sql_database()\n",
    "    create_storage_account_and_file_share()\n",
    "    create_data_factory()\n",
    "    create_ml_workspace()\n",
    "    perform_data_preparation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: SageMaker Data Wrangler, SageMaker Ground Truth.\n",
    "- **Pros**: Easy data preparation and labeling; user-friendly interface for feature engineering.\n",
    "- **Cons**: Can be complex when integrating multiple services.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: Apache Spark, Delta Lake.\n",
    "- **Pros**: Excellent for big data processing; Delta Lake allows ACID transactions.\n",
    "- **Cons**: Requires more setup; may not be ideal for smaller datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Model Development and Experimentation\n",
    "### Azure ML\n",
    "- **Services**: Automated Machine Learning (AutoML), Jupyter notebooks.\n",
    "- **Pros**: User-friendly interface; strong AutoML capabilities.\n",
    "- **Cons**: AutoML can be limited in terms of customization.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: SageMaker Studio, built-in algorithms.\n",
    "- **Pros**: Comprehensive development environment; supports multiple frameworks.\n",
    "- **Cons**: Learning curve for advanced features.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: Collaborative notebooks, MLflow for tracking experiments.\n",
    "- **Pros**: Excellent for collaboration among teams; MLflow integrates well for experiment tracking.\n",
    "- **Cons**: Complexity increases with larger teams and projects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training\n",
    "### Azure ML\n",
    "- **Services**: Compute instances, distributed training.\n",
    "- **Pros**: Scalable training options; easy integration with Azure infrastructure.\n",
    "- **Cons**: Costs can increase with heavy usage.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: Managed training jobs, automatic model tuning (hyperparameter tuning).\n",
    "- **Pros**: Fully managed; supports distributed training.\n",
    "- **Cons**: Pricing complexity can be challenging to estimate.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: Auto-scaling clusters, distributed training.\n",
    "- **Pros**: High scalability; supports Spark MLlib for distributed ML tasks.\n",
    "- **Cons**: Can become expensive for extensive use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Evaluation and Validation\n",
    "### Azure ML\n",
    "- **Services**: Built-in metrics for evaluation, confusion matrix visualizations.\n",
    "- **Pros**: Comprehensive evaluation metrics; user-friendly dashboard.\n",
    "- **Cons**: Some advanced evaluation features may require manual implementation.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: SageMaker Model Monitor.\n",
    "- **Pros**: Automated monitoring of model performance; integrates with SageMaker.\n",
    "- **Cons**: Requires setup to monitor effectively.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: MLflow for tracking metrics.\n",
    "- **Pros**: Great for custom evaluation metrics; flexible tracking.\n",
    "- **Cons**: Requires familiarity with MLflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Deployment and Serving\n",
    "### Azure ML\n",
    "- **Services**: Azure Kubernetes Service (AKS), Azure Container Instances.\n",
    "- **Pros**: Seamless deployment options; easy to scale.\n",
    "- **Cons**: Configuration can be complex for beginners.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: Real-time endpoints, batch transform jobs.\n",
    "- **Pros**: Fully managed endpoints; easy scaling.\n",
    "- **Cons**: Pricing can add up with high traffic.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: Databricks Jobs for scheduling and serving.\n",
    "- **Pros**: Simple integration with existing workflows; supports batch and streaming.\n",
    "- **Cons**: Limited out-of-the-box serving options compared to others.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Performance Monitoring\n",
    "### Azure ML\n",
    "- **Services**: Azure Monitor, built-in monitoring tools.\n",
    "- **Pros**: Integrates well with Azure services for end-to-end monitoring.\n",
    "- **Cons**: Can be costly for extensive monitoring.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: SageMaker Model Monitor, CloudWatch.\n",
    "- **Pros**: Automated monitoring and alerts; comprehensive metrics.\n",
    "- **Cons**: Requires setup and configuration.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: Spark UI, built-in performance monitoring.\n",
    "- **Pros**: Good for tracking Spark jobs; integrates with existing monitoring solutions.\n",
    "- **Cons**: May not provide as detailed metrics for ML models as other platforms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ML Metadata Store\n",
    "### Azure ML\n",
    "- **Services**: Azure ML Workspace.\n",
    "- **Pros**: Organizes experiments, models, and datasets; easy access.\n",
    "- **Cons**: Might lack some advanced features of specialized ML metadata stores.\n",
    "\n",
    "### AWS SageMaker\n",
    "- **Services**: SageMaker Model Registry.\n",
    "- **Pros**: Centralized model management; easy versioning.\n",
    "- **Cons**: Can be limited in features compared to dedicated tools.\n",
    "\n",
    "### Databricks\n",
    "- **Services**: MLflow tracking.\n",
    "- **Pros**: Comprehensive tracking of experiments and models.\n",
    "- **Cons**: Requires understanding of MLflow for effective use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
