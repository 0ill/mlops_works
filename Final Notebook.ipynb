{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Assuming you have a trained model artifact in S3\n",
    "model_data = 's3://your-bucket/model.tar.gz'\n",
    "\n",
    "# Create a SageMaker model\n",
    "model = Model(\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.8.1-cpu-py36-ubuntu18.04',\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    predictor_cls=sagemaker.predictor.Predictor\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name='my-endpoint'\n",
    ")\n",
    "\n",
    "# Make a prediction\n",
    "result = predictor.predict(data)\n",
    "\n",
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the AI Platform\n",
    "aiplatform.init(project='your-project-id')\n",
    "\n",
    "# Create a custom training job\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my-training-job\",\n",
    "    script_path=\"training_script.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-cpu.2-3:latest\"\n",
    ")\n",
    "\n",
    "# Run the training job\n",
    "model = job.run(\n",
    "    dataset=my_dataset,\n",
    "    model_display_name=\"my-model\",\n",
    "    base_output_dir=\"gs://your-bucket/output\"\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "# Connect to your workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Create an experiment\n",
    "experiment = Experiment(workspace=ws, name='my-experiment')\n",
    "\n",
    "# Define the training script\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory='./src',\n",
    "    script='train.py',\n",
    "    compute_target='your-compute-target'\n",
    ")\n",
    "\n",
    "# Submit the experiment\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Register the model\n",
    "model = run.register_model(model_name='my-model', model_path='outputs/model.pkl')\n",
    "\n",
    "# Deploy the model\n",
    "service = Model.deploy(ws, \"myservice\", [model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Assuming you have already trained and logged a model with MLflow\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "client = MlflowClient()\n",
    "#\n",
    "# Get the latest version of the model\n",
    "model_name = \"my-model\"\n",
    "model_version = client.get_latest_versions(model_name, stages=[\"Production\"])[0].version\n",
    "\n",
    "# Create or update a serving endpoint\n",
    "endpoint_name = \"my-model-endpoint\"\n",
    "client.create_model_version_if_not_exists(\n",
    "    name=model_name,\n",
    "    source=f\"models:/{model_name}/{model_version}\",\n",
    "    run_id=None\n",
    ")\n",
    "\n",
    "# Deploy the model to the endpoint\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "\n",
    "# Make a prediction (assuming the endpoint is already active)\n",
    "def query_endpoint(data):\n",
    "    url = f\"https://<your-databricks-instance>/model/{endpoint_name}/1/invocations\"\n",
    "    headers = {'Authorization': f'Bearer {dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()}'}\n",
    "    data_json = json.dumps(data)\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()\n",
    "\n",
    "# Example prediction\n",
    "result = query_endpoint({\"inputs\": [[1.0, 2.0, 3.0]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Container Service\n",
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ecs_client = boto3.client('ecs')\n",
    "\n",
    "# Create a new ECS cluster\n",
    "response = ecs_client.create_cluster(\n",
    "    clusterName='my-cluster'\n",
    ")\n",
    "\n",
    "# Run a task\n",
    "response = ecs_client.run_task(\n",
    "    cluster='my-cluster',\n",
    "    taskDefinition='my-task-definition:1',\n",
    "    count=1,\n",
    "    launchType='FARGATE',\n",
    "    networkConfiguration={\n",
    "        'awsvpcConfiguration': {\n",
    "            'subnets': ['subnet-12345678'],\n",
    "            'assignPublicIp': 'ENABLED'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import run_v2\n",
    "\n",
    "client = run_v2.ServicesClient()\n",
    "\n",
    "# Create a Cloud Run service\n",
    "service = run_v2.Service(\n",
    "    template=run_v2.RevisionTemplate(\n",
    "        containers=[\n",
    "            run_v2.Container(\n",
    "                image=\"gcr.io/my-project/my-image\",\n",
    "                ports=[run_v2.ContainerPort(container_port=8080)]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = client.create_service(\n",
    "    parent=f\"projects/my-project/locations/us-central1\",\n",
    "    service=service,\n",
    "    service_id=\"my-service\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.containerinstance import ContainerInstanceManagementClient\n",
    "from azure.mgmt.containerinstance.models import (ContainerGroup, Container, ResourceRequirements, ResourceRequests)\n",
    "\n",
    "# Create the Container Instance client\n",
    "client = ContainerInstanceManagementClient(credential, subscription_id)\n",
    "\n",
    "# Create container group\n",
    "container_group = ContainerGroup(\n",
    "    location=\"eastus\",\n",
    "    containers=[\n",
    "        Container(\n",
    "            name=\"mycontainer\",\n",
    "            image=\"mcr.microsoft.com/azuredocs/aci-helloworld\",\n",
    "            resources=ResourceRequirements(\n",
    "                requests=ResourceRequests(\n",
    "                    memory_in_gb=1.5,\n",
    "                    cpu=1.0\n",
    "                )\n",
    "            ),\n",
    "            ports=[ContainerPort(port=80)]\n",
    "        )\n",
    "    ],\n",
    "    os_type=\"Linux\",\n",
    "    ip_address=IpAddress(ports=[Port(protocol=\"tcp\", port=80)], type=\"Public\")\n",
    ")\n",
    "\n",
    "# Create the container group\n",
    "client.container_groups.begin_create_or_update(\n",
    "    resource_group_name,\n",
    "    container_group_name,\n",
    "    container_group\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_cli.sdk.api_client import ApiClient\n",
    "from databricks_cli.jobs.api import JobsApi\n",
    "\n",
    "api_client = ApiClient(\n",
    "    host=\"https://your-databricks-instance.cloud.databricks.com\",\n",
    "    token=\"your-access-token\"\n",
    ")\n",
    "\n",
    "jobs_api = JobsApi(api_client)\n",
    "\n",
    "# Create a job that runs a container\n",
    "new_job = {\n",
    "    \"name\": \"Container Job\",\n",
    "    \"new_cluster\": {\n",
    "        \"spark_version\": \"7.3.x-scala2.12\",\n",
    "        \"node_type_id\": \"Standard_DS3_v2\",\n",
    "        \"num_workers\": 2\n",
    "    },\n",
    "    \"docker_image\": {\n",
    "        \"url\": \"your-docker-image-url\",\n",
    "        \"basic_auth\": {\n",
    "            \"username\": \"your-username\",\n",
    "            \"password\": \"your-password\"\n",
    "        }\n",
    "    },\n",
    "    \"spark_python_task\": {\n",
    "        \"python_file\": \"dbfs:/path/to/your/script.py\"\n",
    "    }\n",
    "}\n",
    "\n",
    "job_id = jobs_api.create_job(new_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring and Drift Detection Services\n",
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Enable data capture for your endpoint\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri='s3://your-bucket/captured-data/'\n",
    ")\n",
    "\n",
    "# Create a model monitor\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# Set the baseline\n",
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset='s3://your-bucket/baseline-data/baseline.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    ")\n",
    "\n",
    "# Create a monitoring schedule\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='my-monitoring-schedule',\n",
    "    endpoint_input=endpoint_name,\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression='cron(0 * ? * * *)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project='your-project-id', location='us-central1')\n",
    "\n",
    "# Create a model monitoring job\n",
    "monitoring_job = aiplatform.ModelDeploymentMonitoringJob.create(\n",
    "    display_name=\"my-monitoring-job\",\n",
    "    endpoint=endpoint_name,\n",
    "    schedule=\"0 0 * * *\",  # Run daily at midnight\n",
    "    model_deployment_monitoring_objective_configs=[\n",
    "        aiplatform.ModelDeploymentMonitoringObjectiveConfig(\n",
    "            objective=aiplatform.ModelDeploymentMonitoringObjectiveConfig.Objective.FEATURE_ATTRIBUTION,\n",
    "            explanation_config=aiplatform.explanation.ExplanationConfig(\n",
    "                metadata=aiplatform.explanation.ExplanationMetadata(\n",
    "                    inputs={\n",
    "                        \"feature1\": aiplatform.explanation.ExplanationMetadata.InputMetadata(),\n",
    "                        \"feature2\": aiplatform.explanation.ExplanationMetadata.InputMetadata(),\n",
    "                    },\n",
    "                    outputs={\n",
    "                        \"prediction\": aiplatform.explanation.ExplanationMetadata.OutputMetadata(),\n",
    "                    },\n",
    "                ),\n",
    "                parameters=aiplatform.explanation.ExplanationParameters(\n",
    "                    sampled_shapley_attribution=aiplatform.explanation.ExplanationParameters.SampledShapleyAttribution(\n",
    "                        path_count=10\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    logging_sampling_strategy=aiplatform.ModelDeploymentMonitoringJob.LoggingSamplingStrategy(\n",
    "        random_sampling_percent=10.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The monitoring job is now created and will run according to the schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.datadrift import DataDriftDetector\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Connect to your workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the target dataset and baseline dataset\n",
    "target_dataset = Dataset.get_by_name(ws, 'target_dataset')\n",
    "baseline_dataset = Dataset.get_by_name(ws, 'baseline_dataset')\n",
    "\n",
    "# Create a data drift detector\n",
    "drift_detector = DataDriftDetector.create(\n",
    "    ws, \n",
    "    name='my-drift-detector',\n",
    "    target_dataset=target_dataset,\n",
    "    baseline_dataset=baseline_dataset,\n",
    "    compute_target='your-compute-target',\n",
    "    frequency='Day',\n",
    "    feature_list=['feature1', 'feature2', 'feature3'],\n",
    "    drift_threshold=0.3,\n",
    "    latency=timedelta(hours=24)\n",
    ")\n",
    "\n",
    "# Enable the data drift detector\n",
    "drift_detector.enable(\n",
    "    target_dataset=target_dataset,\n",
    "    schedule_start=datetime.now(),\n",
    "    schedule_end=datetime.now() + timedelta(days=7)\n",
    ")\n",
    "\n",
    "# The detector is now enabled and will run according to the schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import feature_store\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "from databricks.feature_store.online_store_spec import AzureCosmosDBSpec\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Assuming you have a feature store and a model already set up\n",
    "\n",
    "# Set up feature monitoring\n",
    "fs = feature_store.FeatureStoreClient()\n",
    "\n",
    "# Create a feature monitoring configuration\n",
    "monitoring_config = fs.create_monitoring_configuration(\n",
    "    name=\"my-monitoring-config\",\n",
    "    feature_table=feature_table(\n",
    "        name=\"my_feature_table\",\n",
    "        database=\"my_database\"\n",
    "    ),\n",
    "    baseline_table_name=\"baseline_features\",\n",
    "    monitoring_window=\"1d\",\n",
    "    granularities=[\"1h\", \"1d\"],\n",
    ")\n",
    "\n",
    "# Add monitoring rules\n",
    "monitoring_config.add_monitoring_rule(\n",
    "    name=\"feature_drift\",\n",
    "    rule_type=\"drift\",\n",
    "    params={\n",
    "        \"threshold\": 0.1,\n",
    "        \"drift_measure\": \"jensenshannon\"\n",
    "    }\n",
    ")\n",
    "\n",
    "monitoring_config.add_monitoring_rule(\n",
    "    name=\"data_quality\",\n",
    "    rule_type=\"integrity\",\n",
    "    params={\n",
    "        \"min\": 0,\n",
    "        \"max\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save the monitoring configuration\n",
    "fs.save_monitoring_configuration(monitoring_config)\n",
    "\n",
    "# Set up model monitoring\n",
    "client = MlflowClient()\n",
    "\n",
    "# Assuming you have a registered model\n",
    "model_name = \"my-model\"\n",
    "model_version = client.get_latest_versions(model_name, stages=[\"Production\"])[0].version\n",
    "\n",
    "# Log model metrics\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    mlflow.log_metric(\"f1_score\", 0.92)\n",
    "\n",
    "# The monitoring will now run based on the configuration and log results to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete MLops circle\n",
    "### AWS SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, Processor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "\n",
    "# Set up SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Step 1: Data Ingestion\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'your-bucket-name'\n",
    "data_key = 'path/to/your/data.csv'\n",
    "s3_client.download_file(bucket_name, data_key, 'raw_data.csv')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "preprocessing_script = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('/opt/ml/processing/input/raw_data.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save processed data\n",
    "pd.DataFrame(X_train_scaled).to_csv('/opt/ml/processing/output/train_features.csv', index=False)\n",
    "pd.DataFrame(X_test_scaled).to_csv('/opt/ml/processing/output/test_features.csv', index=False)\n",
    "pd.Series(y_train).to_csv('/opt/ml/processing/output/train_labels.csv', index=False)\n",
    "pd.Series(y_test).to_csv('/opt/ml/processing/output/test_labels.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "with open('preprocessing.py', 'w') as f:\n",
    "    f.write(preprocessing_script)\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[ProcessingInput(source='raw_data.csv', destination='/opt/ml/processing/input')],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_features\", source=\"/opt/ml/processing/output/train_features.csv\"),\n",
    "        ProcessingOutput(output_name=\"test_features\", source=\"/opt/ml/processing/output/test_features.csv\"),\n",
    "        ProcessingOutput(output_name=\"train_labels\", source=\"/opt/ml/processing/output/train_labels.csv\"),\n",
    "        ProcessingOutput(output_name=\"test_labels\", source=\"/opt/ml/processing/output/test_labels.csv\")\n",
    "    ],\n",
    "    code='preprocessing.py'\n",
    ")\n",
    "\n",
    "# Step 3: Model Training\n",
    "training_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n-estimators', type=int, default=100)\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    train_features = pd.read_csv(os.path.join(args.train, 'train_features.csv'))\n",
    "    train_labels = pd.read_csv(os.path.join(args.train, 'train_labels.csv'))\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=args.n_estimators)\n",
    "    model.fit(train_features, train_labels)\n",
    "\n",
    "    joblib.dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "\"\"\"\n",
    "\n",
    "with open('train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "estimator = Estimator(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    hyperparameters={\n",
    "        'n-estimators': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs['train_features'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 4: Model Deployment\n",
    "model = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "\n",
    "model_monitor = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f's3://{bucket_name}/model-monitor'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    data_capture_config=model_monitor\n",
    ")\n",
    "\n",
    "# Step 5: Monitoring and Tracking\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "model_monitor.suggest_baseline(\n",
    "    baseline_dataset=preprocessing_step.properties.ProcessingOutputConfig.Outputs['test_features'].S3Output.S3Uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f's3://{bucket_name}/model-monitor/baseline',\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "monitoring_schedule_name = 'my-monitoring-schedule'\n",
    "model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitoring_schedule_name,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f's3://{bucket_name}/model-monitor/output',\n",
    "    statistics=model_monitor.baseline_statistics(),\n",
    "    constraints=model_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression='cron(0 * ? * * *)'  # Run hourly\n",
    ")\n",
    "\n",
    "# Create and execute the pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"MLOpsPipeline\",\n",
    "    steps=[preprocessing_step, training_step]\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "\n",
    "# Get the status of the pipeline execution\n",
    "execution.describe()\n",
    "\n",
    "# Clean up resources\n",
    "predictor.delete_endpoint()\n",
    "execution.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes you have the following structure:\n",
    "# \n",
    "# project_root/\n",
    "# ├── .github/\n",
    "# │   └── workflows/\n",
    "# │       └── mlops_pipeline.yml\n",
    "# ├── src/\n",
    "# │   ├── preprocess.py\n",
    "# │   ├── train.py\n",
    "# │   └── inference.py\n",
    "# ├── tests/\n",
    "# │   └── test_model.py\n",
    "# ├── Dockerfile\n",
    "# ├── requirements.txt\n",
    "# └── main.tf\n",
    "\n",
    "# File: src/preprocess.py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "def preprocess_data(bucket_name, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save preprocessed data back to S3\n",
    "    train_data = pd.DataFrame(X_train_scaled).join(y_train.reset_index(drop=True))\n",
    "    test_data = pd.DataFrame(X_test_scaled).join(y_test.reset_index(drop=True))\n",
    "    \n",
    "    train_buffer = io.StringIO()\n",
    "    test_buffer = io.StringIO()\n",
    "    train_data.to_csv(train_buffer, index=False)\n",
    "    test_data.to_csv(test_buffer, index=False)\n",
    "    \n",
    "    s3.put_object(Bucket=bucket_name, Key='processed/train.csv', Body=train_buffer.getvalue())\n",
    "    s3.put_object(Bucket=bucket_name, Key='processed/test.csv', Body=test_buffer.getvalue())\n",
    "\n",
    "# File: src/train.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-data', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    data = pd.read_csv(f'{args.train_data}/train.csv')\n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    joblib.dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "\n",
    "# File: src/inference.py\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'application/json':\n",
    "        data = pd.read_json(request_body, orient='split')\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    predictions = model.predict(input_data)\n",
    "    return predictions\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    if response_content_type == 'application/json':\n",
    "        return pd.Series(prediction).to_json(orient='records')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {response_content_type}\")\n",
    "\n",
    "# File: Dockerfile\n",
    "FROM python:3.8-slim\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY src /opt/ml/code\n",
    "WORKDIR /opt/ml/code\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\n",
    "# File: .github/workflows/mlops_pipeline.yml\n",
    "name: MLOps Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.8\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "    - name: Run tests\n",
    "      run: python -m pytest tests/\n",
    "\n",
    "  build-and-push:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Configure AWS credentials\n",
    "      uses: aws-actions/configure-aws-credentials@v1\n",
    "      with:\n",
    "        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "        aws-region: us-west-2\n",
    "    - name: Login to Amazon ECR\n",
    "      id: login-ecr\n",
    "      uses: aws-actions/amazon-ecr-login@v1\n",
    "    - name: Build, tag, and push image to Amazon ECR\n",
    "      env:\n",
    "        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n",
    "        ECR_REPOSITORY: my-ecr-repo\n",
    "        IMAGE_TAG: ${{ github.sha }}\n",
    "      run: |\n",
    "        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .\n",
    "        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n",
    "\n",
    "  deploy:\n",
    "    needs: build-and-push\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Configure AWS credentials\n",
    "      uses: aws-actions/configure-aws-credentials@v1\n",
    "      with:\n",
    "        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "        aws-region: us-west-2\n",
    "    - name: Setup Terraform\n",
    "      uses: hashicorp/setup-terraform@v1\n",
    "    - name: Terraform Init\n",
    "      run: terraform init\n",
    "    - name: Terraform Apply\n",
    "      run: terraform apply -auto-approve\n",
    "\n",
    "# File: main.tf\n",
    "terraform {\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 3.0\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = \"us-west-2\"\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket\" \"data_bucket\" {\n",
    "  bucket = \"my-mlops-data-bucket\"\n",
    "  acl    = \"private\"\n",
    "}\n",
    "\n",
    "resource \"aws_sagemaker_notebook_instance\" \"ni\" {\n",
    "  name                    = \"my-notebook-instance\"\n",
    "  role_arn                = aws_iam_role.sagemaker_role.arn\n",
    "  instance_type           = \"ml.t2.medium\"\n",
    "  lifecycle {\n",
    "    create_before_destroy = true\n",
    "  }\n",
    "}\n",
    "\n",
    "resource \"aws_iam_role\" \"sagemaker_role\" {\n",
    "  name               = \"sagemaker-execution-role\"\n",
    "  assume_role_policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [\n",
    "      {\n",
    "        Action = \"sts:AssumeRole\"\n",
    "        Effect = \"Allow\"\n",
    "        Principal = {\n",
    "          Service = \"sagemaker.amazonaws.com\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  })\n",
    "}\n",
    "\n",
    "resource \"aws_iam_role_policy_attachment\" \"sagemaker_full_access\" {\n",
    "  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
    "  role       = aws_iam_role.sagemaker_role.name\n",
    "}\n",
    "\n",
    "# SageMaker model and endpoint resources would be defined here\n",
    "\n",
    "# File: tests/test_model.py\n",
    "import pytest\n",
    "from src.train import RandomForestClassifier\n",
    "\n",
    "def test_model_creation():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    assert model.n_estimators == 100\n",
    "\n",
    "# Main Python script to orchestrate the MLOps pipeline\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "\n",
    "def main():\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    bucket = 'my-mlops-data-bucket'\n",
    "    prefix = 'sagemaker/mlops-demo'\n",
    "\n",
    "    # Data Ingestion and Preprocessing\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version='0.23-1',\n",
    "        role=sagemaker.get_execution_role(),\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        instance_count=1\n",
    "    )\n",
    "\n",
    "    sklearn_processor.run(\n",
    "        code='src/preprocess.py',\n",
    "        inputs=[ProcessingInput(source=f's3://{bucket}/raw/data.csv', destination='/opt/ml/processing/input')],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name='train', source='/opt/ml/processing/train'),\n",
    "            ProcessingOutput(output_name='test', source='/opt/ml/processing/test')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Model Training\n",
    "    estimator = Estimator(\n",
    "        image_uri=f\"{account}.dkr.ecr.{region}.amazonaws.com/my-ecr-repo:{github.sha}\",\n",
    "        role=sagemaker.get_execution_role(),\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        output_path=f's3://{bucket}/{prefix}/output'\n",
    "    )\n",
    "\n",
    "    estimator.fit({'train': f's3://{bucket}/{prefix}/train'})\n",
    "\n",
    "    # Model Deployment\n",
    "    predictor = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        data_capture_config=DataCaptureConfig(\n",
    "            enable_capture=True,\n",
    "            sampling_percentage=100,\n",
    "            destination_s3_uri=f's3://{bucket}/{prefix}/data-capture'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Model Monitoring\n",
    "    model_monitor = DefaultModelMonitor(\n",
    "        role=sagemaker.get_execution_role(),\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        volume_size_in_gb=20,\n",
    "        max_runtime_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    model_monitor.suggest_baseline(\n",
    "        baseline_dataset=f's3://{bucket}/{prefix}/test/test.csv',\n",
    "        dataset_format=DatasetFormat.csv(header=True),\n",
    "        output_s3_uri=f's3://{bucket}/{prefix}/monitoring/baseline',\n",
    "        wait=True\n",
    "    )\n",
    "\n",
    "    monitoring_schedule_name = 'my-monitoring-schedule'\n",
    "    model_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=monitoring_schedule_name,\n",
    "        endpoint_input=predictor.endpoint_name,\n",
    "        output_s3_uri=f's3://{bucket}/{prefix}/monitoring/output',\n",
    "        statistics=model_monitor.baseline_statistics(),\n",
    "        constraints=model_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression='cron(0 * ? * * *)'  # Run hourly\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Step 1: Set up Azure ML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Step 2: Data Ingestion\n",
    "datastore = ws.get_default_datastore()\n",
    "dataset = Dataset.File.from_files(path=(datastore, 'path/to/your/data.csv'))\n",
    "dataset = dataset.register(workspace=ws, name='raw_data', create_new_version=True)\n",
    "\n",
    "# Step 3: Create Compute Target\n",
    "compute_name = \"cpu-cluster\"\n",
    "vm_size = \"Standard_DS3_v2\"\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "else:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size, max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Step 4: Define Environment\n",
    "sklearn_env = Environment(\"sklearn-env\")\n",
    "sklearn_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn', 'pandas', 'numpy'])\n",
    "\n",
    "# Step 5: Data Preprocessing\n",
    "preprocess_step = PythonScriptStep(\n",
    "    name=\"preprocess_data\",\n",
    "    source_directory=\".\",\n",
    "    script_name=\"preprocess.py\",\n",
    "    compute_target=compute_target,\n",
    "    inputs=[dataset.as_named_input('raw_data')],\n",
    "    outputs=[PipelineData(\"processed_data\", datastore=datastore)],\n",
    "    runconfig=RunConfiguration(environment=sklearn_env)\n",
    ")\n",
    "\n",
    "# Step 6: Model Training\n",
    "train_step = PythonScriptStep(\n",
    "    name=\"train_model\",\n",
    "    source_directory=\".\",\n",
    "    script_name=\"train.py\",\n",
    "    compute_target=compute_target,\n",
    "    inputs=[preprocess_step.outputs['processed_data']],\n",
    "    outputs=[PipelineData(\"model\", datastore=datastore)],\n",
    "    runconfig=RunConfiguration(environment=sklearn_env)\n",
    ")\n",
    "\n",
    "# Step 7: Create and Run Pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[preprocess_step, train_step])\n",
    "pipeline_run = Experiment(ws, 'MLOpsPipeline').submit(pipeline)\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion()\n",
    "\n",
    "# Step 8: Register the Model\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_path=\"outputs/model\",\n",
    "                       model_name=\"sklearn_model\",\n",
    "                       tags={\"data\": \"tabular\", \"type\": \"classification\"},\n",
    "                       description=\"Sklearn model trained on tabular data\")\n",
    "\n",
    "# Step 9: Deploy the Model\n",
    "inference_config = InferenceConfig(\n",
    "    entry_script=\"score.py\",\n",
    "    environment=sklearn_env\n",
    ")\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1,\n",
    "    auth_enabled=True,\n",
    "    enable_app_insights=True\n",
    ")\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='sklearn-service',\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "# Step 10: Enable Data Collection for Model Monitoring\n",
    "service.update(enable_app_insights=True, collect_model_data=True)\n",
    "\n",
    "print(service.scoring_uri)\n",
    "\n",
    "# Contents of preprocess.py\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_data', type=str)\n",
    "parser.add_argument('--output_data', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load data\n",
    "data = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Preprocess\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save processed data\n",
    "os.makedirs(args.output_data, exist_ok=True)\n",
    "pd.DataFrame(X_train_scaled).to_csv(os.path.join(args.output_data, 'train_features.csv'), index=False)\n",
    "pd.DataFrame(X_test_scaled).to_csv(os.path.join(args.output_data, 'test_features.csv'), index=False)\n",
    "pd.Series(y_train).to_csv(os.path.join(args.output_data, 'train_labels.csv'), index=False)\n",
    "pd.Series(y_test).to_csv(os.path.join(args.output_data, 'test_labels.csv'), index=False)\n",
    "\"\"\"\n",
    "\n",
    "# Contents of train.py\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_data', type=str)\n",
    "parser.add_argument('--output_data', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load data\n",
    "train_features = pd.read_csv(os.path.join(args.input_data, 'train_features.csv'))\n",
    "train_labels = pd.read_csv(os.path.join(args.input_data, 'train_labels.csv'))\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Save model\n",
    "os.makedirs(args.output_data, exist_ok=True)\n",
    "joblib.dump(model, os.path.join(args.output_data, 'model.pkl'))\n",
    "\"\"\"\n",
    "\n",
    "# Contents of score.py\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('sklearn_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = pd.DataFrame.from_dict(data)\n",
    "        result = model.predict(data)\n",
    "        return json.dumps({\"result\": result.tolist()})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes you have the following structure:\n",
    "# \n",
    "# project_root/\n",
    "# ├── .github/\n",
    "# │   └── workflows/\n",
    "# │       └── mlops_pipeline.yml\n",
    "# ├── src/\n",
    "# │   ├── preprocess.py\n",
    "# │   ├── train.py\n",
    "# │   └── score.py\n",
    "# ├── tests/\n",
    "# │   └── test_model.py\n",
    "# ├── Dockerfile\n",
    "# ├── environment.yml\n",
    "# └── main.tf\n",
    "\n",
    "# File: src/preprocess.py\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data():\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    # Get the input dataset by name\n",
    "    dataset = run.input_datasets['raw_data']\n",
    "    data = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    train_data = pd.DataFrame(X_train_scaled).join(y_train.reset_index(drop=True))\n",
    "    test_data = pd.DataFrame(X_test_scaled).join(y_test.reset_index(drop=True))\n",
    "    \n",
    "    train_data.to_csv('outputs/train.csv', index=False)\n",
    "    test_data.to_csv('outputs/test.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()\n",
    "\n",
    "# File: src/train.py\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "def train_model():\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    # Get the input dataset by name\n",
    "    dataset = run.input_datasets['train_data']\n",
    "    data = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Log metrics\n",
    "    run.log('n_estimators', model.n_estimators)\n",
    "    run.log('accuracy', model.score(X, y))\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, 'outputs/model.joblib')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "\n",
    "# File: src/score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('sklearn_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = pd.DataFrame.from_dict(data)\n",
    "        result = model.predict(data)\n",
    "        return json.dumps({\"result\": result.tolist()})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "# File: Dockerfile\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
    "\n",
    "ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/sklearn-env\n",
    "\n",
    "# Create conda environment\n",
    "COPY environment.yml /tmp/conda_dependencies.yml\n",
    "RUN conda env create -p $AZUREML_CONDA_ENVIRONMENT_PATH -f /tmp/conda_dependencies.yml && \\\n",
    "    rm -rf /opt/conda/pkgs && \\\n",
    "    conda clean -a -y\n",
    "\n",
    "# Activate conda environment\n",
    "ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\n",
    "\n",
    "# Copy files\n",
    "COPY src /azureml-app\n",
    "\n",
    "# File: environment.yml\n",
    "name: sklearn-env\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "\n",
    "# File: .github/workflows/mlops_pipeline.yml\n",
    "name: MLOps Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.8\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "    - name: Run tests\n",
    "      run: python -m pytest tests/\n",
    "\n",
    "  build-and-push:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: 'Docker login to ACR'\n",
    "      uses: azure/docker-login@v1\n",
    "      with:\n",
    "        login-server: ${{ secrets.ACR_LOGIN_SERVER }}\n",
    "        username: ${{ secrets.ACR_USERNAME }}\n",
    "        password: ${{ secrets.ACR_PASSWORD }}\n",
    "    - name: Build and push image to ACR\n",
    "      run: |\n",
    "        docker build -t ${{ secrets.ACR_LOGIN_SERVER }}/mlops-image:${{ github.sha }} .\n",
    "        docker push ${{ secrets.ACR_LOGIN_SERVER }}/mlops-image:${{ github.sha }}\n",
    "\n",
    "  deploy:\n",
    "    needs: build-and-push\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Azure Login\n",
    "      uses: azure/login@v1\n",
    "      with:\n",
    "        creds: ${{ secrets.AZURE_CREDENTIALS }}\n",
    "    - name: Setup Terraform\n",
    "      uses: hashicorp/setup-terraform@v1\n",
    "    - name: Terraform Init\n",
    "      run: terraform init\n",
    "    - name: Terraform Apply\n",
    "      run: terraform apply -auto-approve\n",
    "\n",
    "# File: main.tf\n",
    "terraform {\n",
    "  required_providers {\n",
    "    azurerm = {\n",
    "      source  = \"hashicorp/azurerm\"\n",
    "      version = \"~> 2.65\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"azurerm\" {\n",
    "  features {}\n",
    "}\n",
    "\n",
    "resource \"azurerm_resource_group\" \"rg\" {\n",
    "  name     = \"mlops-resources\"\n",
    "  location = \"East US\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_storage_account\" \"storage\" {\n",
    "  name                     = \"mlopsstorage\"\n",
    "  resource_group_name      = azurerm_resource_group.rg.name\n",
    "  location                 = azurerm_resource_group.rg.location\n",
    "  account_tier             = \"Standard\"\n",
    "  account_replication_type = \"LRS\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_machine_learning_workspace\" \"mlw\" {\n",
    "  name                    = \"mlops-workspace\"\n",
    "  location                = azurerm_resource_group.rg.location\n",
    "  resource_group_name     = azurerm_resource_group.rg.name\n",
    "  application_insights_id = azurerm_application_insights.ai.id\n",
    "  key_vault_id            = azurerm_key_vault.kv.id\n",
    "  storage_account_id      = azurerm_storage_account.storage.id\n",
    "\n",
    "  identity {\n",
    "    type = \"SystemAssigned\"\n",
    "  }\n",
    "}\n",
    "\n",
    "resource \"azurerm_application_insights\" \"ai\" {\n",
    "  name                = \"mlops-app-insights\"\n",
    "  location            = azurerm_resource_group.rg.location\n",
    "  resource_group_name = azurerm_resource_group.rg.name\n",
    "  application_type    = \"web\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_key_vault\" \"kv\" {\n",
    "  name                = \"mlops-keyvault\"\n",
    "  location            = azurerm_resource_group.rg.location\n",
    "  resource_group_name = azurerm_resource_group.rg.name\n",
    "  tenant_id           = data.azurerm_client_config.current.tenant_id\n",
    "  sku_name            = \"standard\"\n",
    "}\n",
    "\n",
    "# File: tests/test_model.py\n",
    "import pytest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def test_model_creation():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    assert model.n_estimators == 100\n",
    "\n",
    "# Main Python script to orchestrate the MLOps pipeline\n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "def main():\n",
    "    # Connect to workspace\n",
    "    ws = Workspace.from_config()\n",
    "\n",
    "    # Create compute target\n",
    "    compute_name = \"cpu-cluster\"\n",
    "    if compute_name not in ws.compute_targets:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS3_v2', \n",
    "                                                               max_nodes=4)\n",
    "        compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "        compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "    # Define environment\n",
    "    env = Environment.from_conda_specification(name=\"sklearn-env\", file_path=\"environment.yml\")\n",
    "\n",
    "    # Data Ingestion and Preprocessing\n",
    "    dataset = Dataset.get_by_name(ws, name='raw_data')\n",
    "    preprocess_step = PythonScriptStep(\n",
    "        name=\"Preprocess Data\",\n",
    "        source_directory=\"src\",\n",
    "        script_name=\"preprocess.py\",\n",
    "        compute_target=compute_target,\n",
    "        inputs=[dataset.as_named_input('raw_data')],\n",
    "        outputs=[PipelineData(\"processed_data\", datastore=ws.get_default_datastore())],\n",
    "        runconfig=RunConfiguration(conda_dependencies=CondaDependencies.create(conda_packages=['scikit-learn', 'pandas']))\n",
    "    )\n",
    "\n",
    "    # Model Training\n",
    "    train_step = PythonScriptStep(\n",
    "        name=\"Train Model\",\n",
    "        source_directory=\"src\",\n",
    "        script_name=\"train.py\",\n",
    "        compute_target=compute_target,\n",
    "        inputs=[preprocess_step.outputs['processed_data'].as_input('train_data')],\n",
    "        runconfig=RunConfiguration(conda_dependencies=CondaDependencies.create(conda_packages=['scikit-learn', 'pandas']))\n",
    "    )\n",
    "\n",
    "    # Create and run pipeline\n",
    "    pipeline = Pipeline(workspace=ws, steps=[preprocess_step, train_step])\n",
    "    pipeline_run = Experiment(ws, 'MLOpsPipeline').submit(pipeline)\n",
    "    pipeline_run.wait_for_completion()\n",
    "\n",
    "    # Register the model\n",
    "    model = Model.register(workspace=ws,\n",
    "                           model_path=\"outputs/model.joblib\",\n",
    "                           model_name=\"sklearn_model\",\n",
    "                           tags={\"data\": \"tabular\", \"type\": \"classification\"},\n",
    "                           description=\"Random Forest model for classification\")\n",
    "\n",
    "    # Deploy the model\n",
    "    inference_config = InferenceConfig(\n",
    "        entry_script=\"score.py\",\n",
    "        source_directory=\"src\",\n",
    "        environment=env\n",
    "    )\n",
    "\n",
    "    deployment_config = AciWebservice.deploy_configuration(\n",
    "        cpu_cores=1,\n",
    "        memory_gb=1,\n",
    "        auth_enabled=True,\n",
    "        enable_app_insights=True,\n",
    "        collect_model_data=True\n",
    "    )\n",
    "\n",
    "    service = Model.deploy(workspace=ws,\n",
    "                           name='sklearn-service',\n",
    "                           models=[model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config=deployment_config)\n",
    "\n",
    "    service.wait_for_deployment(show_output=True)\n",
    "\n",
    "    print(f\"Deployment succeeded. Scoring URI: {service.scoring_uri}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook\n",
    "# This notebook assumes you're running it in a Databricks environment\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as SklearnRFC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MLOpsPipeline\").getOrCreate()\n",
    "\n",
    "# Step 1: Data Ingestion\n",
    "# Assuming data is stored in Databricks File System (DBFS)\n",
    "data = spark.read.csv(\"/dbfs/FileStore/path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "feature_columns = data.columns[:-1]  # Assuming last column is the target\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Step 3: Model Training\n",
    "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"scaled_features\", numTrees=100)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Step 5: MLflow Tracking\n",
    "with mlflow.start_run(run_name=\"MLOpsPipeline\") as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_trees\", 100)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.spark.log_model(model, \"spark_model\")\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = model.stages[-1].featureImportances\n",
    "    mlflow.log_param(\"feature_importance\", feature_importance.toArray().tolist())\n",
    "\n",
    "# Step 6: Model Registration\n",
    "model_name = \"RandomForestClassifier\"\n",
    "model_version = mlflow.register_model(f\"runs:/{run.info.run_id}/spark_model\", model_name)\n",
    "\n",
    "# Step 7: Model Deployment\n",
    "# In Databricks, you can use MLflow Model Serving for deployment\n",
    "# This is typically done through the Databricks UI or REST API\n",
    "# Here's a programmatic example of how to load and use the model:\n",
    "\n",
    "loaded_model = mlflow.spark.load_model(f\"models:/{model_name}/{model_version.version}\")\n",
    "\n",
    "# Step 8: Inference\n",
    "# Example of how to use the loaded model for inference\n",
    "new_data = spark.read.csv(\"/dbfs/FileStore/path/to/new/data.csv\", header=True, inferSchema=True)\n",
    "predictions = loaded_model.transform(new_data)\n",
    "\n",
    "# Step 9: Monitoring and Tracking\n",
    "# Databricks integrates with MLflow for experiment tracking\n",
    "# You can use MLflow's tracking UI to monitor model performance\n",
    "# For more advanced monitoring, you can set up custom dashboards in Databricks\n",
    "\n",
    "# Here's an example of how to log predictions and actual values for monitoring\n",
    "def log_predictions(batch_df, batch_id):\n",
    "    with mlflow.start_run(run_name=f\"prediction_logging_{batch_id}\"):\n",
    "        for _, row in batch_df.iterrows():\n",
    "            mlflow.log_metric(\"prediction\", row['prediction'])\n",
    "            mlflow.log_metric(\"actual\", row['target'])\n",
    "\n",
    "# Simulate batch predictions\n",
    "predictions_pd = predictions.select(\"prediction\", \"target\").toPandas()\n",
    "for i in range(0, len(predictions_pd), 100):\n",
    "    batch = predictions_pd.iloc[i:i+100]\n",
    "    log_predictions(batch, i//100)\n",
    "\n",
    "# Step 10: Model Retraining\n",
    "# Here's an example of how you might set up automated retraining\n",
    "\n",
    "def retrain_model():\n",
    "    # Fetch new data\n",
    "    new_data = spark.read.csv(\"/dbfs/FileStore/path/to/updated/data.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Preprocess\n",
    "    assembled = assembler.transform(new_data)\n",
    "    scaled = scaler.transform(assembled)\n",
    "    \n",
    "    # Retrain\n",
    "    new_model = rf.fit(scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    new_predictions = new_model.transform(scaled)\n",
    "    new_accuracy = evaluator.evaluate(new_predictions)\n",
    "    \n",
    "    # Log new model\n",
    "    with mlflow.start_run(run_name=\"ModelRetraining\") as run:\n",
    "        mlflow.log_metric(\"accuracy\", new_accuracy)\n",
    "        mlflow.spark.log_model(new_model, \"retrained_model\")\n",
    "        \n",
    "        # Register new model version\n",
    "        new_model_version = mlflow.register_model(f\"runs:/{run.info.run_id}/retrained_model\", model_name)\n",
    "    \n",
    "    return new_model_version\n",
    "\n",
    "# You can schedule this function to run periodically using Databricks Jobs\n",
    "\n",
    "# Additional MLOps Considerations:\n",
    "# 1. Version Control: Use Git integration in Databricks for version control of notebooks and scripts\n",
    "# 2. CI/CD: Utilize Databricks Repos and integrate with your CI/CD pipelines\n",
    "# 3. Data Validation: Implement data validation checks before model training\n",
    "# 4. A/B Testing: Use Databricks' experimentation features for A/B testing of models\n",
    "# 5. Model Governance: Leverage MLflow's model registry for model lineage and governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud.aiplatform.training_jobs import CustomTrainingJob\n",
    "from google.cloud.aiplatform_v1.types import DeployedModel\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.compiler import Compiler\n",
    "\n",
    "# Set up Google Cloud project and region\n",
    "PROJECT_ID = 'your-project-id'\n",
    "REGION = 'us-central1'\n",
    "BUCKET_NAME = 'your-bucket-name'\n",
    "\n",
    "# Step 1: Data Ingestion\n",
    "def ingest_data():\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob('path/to/your/data.csv')\n",
    "    blob.download_to_filename('data.csv')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "@dsl.component\n",
    "def preprocess_data(data_path: str, output_path: str):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    pd.DataFrame(X_train_scaled).to_csv(f'{output_path}/train_features.csv', index=False)\n",
    "    pd.DataFrame(X_test_scaled).to_csv(f'{output_path}/test_features.csv', index=False)\n",
    "    pd.Series(y_train).to_csv(f'{output_path}/train_labels.csv', index=False)\n",
    "    pd.Series(y_test).to_csv(f'{output_path}/test_labels.csv', index=False)\n",
    "\n",
    "# Step 3: Model Training\n",
    "@dsl.component\n",
    "def train_model(train_features: str, train_labels: str, model_path: str):\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import joblib\n",
    "    \n",
    "    X_train = pd.read_csv(train_features)\n",
    "    y_train = pd.read_csv(train_labels)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "@dsl.component\n",
    "def evaluate_model(test_features: str, test_labels: str, model_path: str) -> float:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    X_test = pd.read_csv(test_features)\n",
    "    y_test = pd.read_csv(test_labels)\n",
    "    \n",
    "    model = joblib.load(model_path)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Step 5: Create Pipeline\n",
    "@dsl.pipeline(name='MLOpsPipeline')\n",
    "def mlops_pipeline():\n",
    "    preprocess_task = preprocess_data(data_path='data.csv', output_path='preprocessed')\n",
    "    train_task = train_model(\n",
    "        train_features=preprocess_task.outputs['train_features'],\n",
    "        train_labels=preprocess_task.outputs['train_labels'],\n",
    "        model_path='model.joblib'\n",
    "    )\n",
    "    evaluate_task = evaluate_model(\n",
    "        test_features=preprocess_task.outputs['test_features'],\n",
    "        test_labels=preprocess_task.outputs['test_labels'],\n",
    "        model_path=train_task.outputs['model_path']\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler = Compiler()\n",
    "compiler.compile(mlops_pipeline, 'pipeline.json')\n",
    "\n",
    "# Step 6: Run Pipeline\n",
    "def run_pipeline():\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    job = pipeline_jobs.PipelineJob(\n",
    "        display_name='MLOpsPipeline',\n",
    "        template_path='pipeline.json',\n",
    "        pipeline_root=f'gs://{BUCKET_NAME}/pipeline_root'\n",
    "    )\n",
    "    job.submit()\n",
    "\n",
    "# Step 7: Model Deployment\n",
    "def deploy_model():\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name='RandomForestModel',\n",
    "        artifact_uri=f'gs://{BUCKET_NAME}/model.joblib',\n",
    "        serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/sklearn-cpu.0-23:latest'\n",
    "    )\n",
    "    endpoint = model.deploy(\n",
    "        machine_type='n1-standard-2',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=3,\n",
    "        traffic_split={\"0\": 100}\n",
    "    )\n",
    "    return endpoint\n",
    "\n",
    "# Step 8: Model Monitoring\n",
    "def setup_monitoring(endpoint):\n",
    "    monitoring_job = aiplatform.ModelDeploymentMonitoringJob.create(\n",
    "        display_name='ModelMonitoring',\n",
    "        endpoint=endpoint,\n",
    "        schedule='0 * * * *',  # Run hourly\n",
    "        metrics=['accuracy'],\n",
    "        analysis_instance_schema_uri='gs://google-cloud-aiplatform/schema/predict/prediction_v1.schema.json',\n",
    "        sample_rate=1.0,\n",
    "        monitoring_interval='3600s',\n",
    "        logging_sampling_strategy=DeployedModel.LoggingStrategy(sampling_rate=1.0)\n",
    "    )\n",
    "    return monitoring_job\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    ingest_data()\n",
    "    run_pipeline()\n",
    "    endpoint = deploy_model()\n",
    "    monitoring_job = setup_monitoring(endpoint)\n",
    "\n",
    "# Additional MLOps considerations:\n",
    "# 1. Version Control: Use Cloud Source Repositories or integrate with GitHub\n",
    "# 2. CI/CD: Use Cloud Build for CI/CD pipelines\n",
    "# 3. Feature Store: Consider using Vertex AI Feature Store for feature management\n",
    "# 4. Model Registry: Use Vertex AI Model Registry for model versioning and lineage\n",
    "# 5. A/B Testing: Implement A/B testing using traffic splits in Vertex AI endpoints\n",
    "# 6. Explainability: Use Vertex AI Explainable AI for model interpretability\n",
    "# 7. Data Validation: Implement TensorFlow Data Validation (TFDV) for data quality checks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
